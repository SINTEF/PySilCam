{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the TFLearn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_preprocessing import ImagePreprocessing\n",
    "from tflearn.data_augmentation import ImageAugmentation\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from skimage.io import imread\n",
    "import skimage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_PATH = 'C:\\\\Users\\\\williamrobbn\\\\SilCam\\\\PySilCam\\\\pysilcam-testdata\\\\unittest-data\\\\silcam_classification_database'\n",
    "MODEL_PATH = 'C:\\\\Users\\\\williamrobbn\\\\SilCam\\\\PySilCam\\\\pysilcam-testdata\\\\tflmodel\\\\particle-classifier.tfl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_labels(model_path='/mnt/ARRAY/classifier/model/particle-classifier.tfl'):\n",
    "    path, filename = os.path.split(model_path)\n",
    "    header = pd.read_csv(os.path.join(path, 'header.tfl.txt'))\n",
    "    class_labels = header.columns\n",
    "    return class_labels\n",
    "\n",
    "def load_model(model_path='/mnt/ARRAY/classifier/model/particle-classifier.tfl'):\n",
    "    path, filename = os.path.split(model_path)\n",
    "    header = pd.read_csv(os.path.join(path, 'header.tfl.txt'))\n",
    "    OUTPUTS = len(header.columns)\n",
    "    class_labels = header.columns\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Same network definition as in tfl_tools scripts\n",
    "    img_prep = ImagePreprocessing()\n",
    "    img_prep.add_featurewise_zero_center()\n",
    "    img_prep.add_featurewise_stdnorm()\n",
    "    img_aug = ImageAugmentation()\n",
    "    img_aug.add_random_flip_leftright()\n",
    "    img_aug.add_random_rotation(max_angle=25.)\n",
    "    img_aug.add_random_blur(sigma_max=3.)\n",
    "\n",
    "    input1 = input_data(shape=[None, 32, 32, 3],\n",
    "                         data_preprocessing=img_prep,\n",
    "                         data_augmentation=img_aug)\n",
    "    conv1 = conv_2d(input1, 32, 3, activation='relu')\n",
    "    pool1 = max_pool_2d(conv1, 2)\n",
    "    conv2 = conv_2d(pool1, 64, 3, activation='relu')\n",
    "    conv3 = conv_2d(conv2, 64, 3, activation='relu')\n",
    "    conv4 = conv_2d(conv3, 64, 3, activation='relu')\n",
    "    conv5 = conv_2d(conv4, 64, 3, activation='relu')\n",
    "    conv6 = conv_2d(conv5, 64, 3, activation='relu')\n",
    "    pool2 = max_pool_2d(conv6, 2)\n",
    "    fc1 = fully_connected(pool2, 512, activation='relu')\n",
    "    drop1 = dropout(fc1, 0.75)\n",
    "    fc2 = fully_connected(drop1, OUTPUTS, activation='softmax')\n",
    "    network = regression(fc2, optimizer='adam',\n",
    "                         loss='categorical_crossentropy',\n",
    "                         learning_rate=0.001)\n",
    "\n",
    "    model = tflearn.DNN(network, tensorboard_verbose=0,\n",
    "            checkpoint_path=model_path)\n",
    "    model.load(model_path)\n",
    "\n",
    "    return model, class_labels, input1, conv1, pool1, conv2, conv3, conv4, conv5, conv6, pool2, fc1, drop1, fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the unit test that checks the accuracy per class.\n",
    "\n",
    "The keras version is lightly different, since I did the image pre-processing outside the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfl_correct_positives(category, model):\n",
    "    files = glob.glob(os.path.join(DATABASE_PATH, category, '*.tiff'))\n",
    "\n",
    "    failed = 0\n",
    "    for file in files:\n",
    "        img = imread(file)  # load ROI\n",
    "        img = scipy.misc.imresize(img, (32, 32), interp=\"bicubic\").astype(np.float32, casting='unsafe')\n",
    "        prediction = model.predict(np.expand_dims(img, 0))  # run prediction from silcam_classify\n",
    "        ind = np.argmax(prediction)  # find the highest score\n",
    "        if not class_labels[ind] == category:\n",
    "            failed += 1\n",
    "    success = 100 - (failed / len(files)) * 100\n",
    "    return success\n",
    "\n",
    "\n",
    "def keras_correct_positives(category, model):\n",
    "    files = glob.glob(os.path.join(DATABASE_PATH, category, '*.tiff'))\n",
    "\n",
    "    failed = 0\n",
    "    for file in files:\n",
    "        img = imread(file)  # load ROI\n",
    "        img = scipy.misc.imresize(img, (32, 32), interp=\"bicubic\").astype(np.float32, casting='unsafe')\n",
    "        img = (img - 195.17760394934288) / 56.10742134506719\n",
    "        prediction = model.predict(np.expand_dims(img, 0))\n",
    "        ind = np.argmax(prediction)  # find the highest score\n",
    "        if not class_labels[ind] == category:\n",
    "            failed += 1\n",
    "    success = 100 - (failed / len(files)) * 100\n",
    "    return success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = get_class_labels(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, class_labels, input1, conv1, pool1, conv2, conv3, conv4, conv5, conv6, pool2, fc1, drop1, fc2 = load_model(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the loaded TFL model that it give the accuracies expected.\n",
    "\n",
    "This take a couple mins to run, and should give:\n",
    "```\n",
    "oil 99.40387481371089\n",
    "other 98.08389435525635\n",
    "bubble 99.96206373292868\n",
    "faecal_pellets 96.69260700389106\n",
    "copepod 99.39117199391173\n",
    "diatom_chain 98.3529411764706\n",
    "oily_gas 97.7035490605428\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in class_labels:\n",
    "#     acc = tfl_correct_positives(c, model)\n",
    "#     print(c, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the weights, transfer to Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the weights for a specific layer (.W gives weights, .b give biases):\n",
    "a = model.get_weights(conv1.W)\n",
    "print(type(a), a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weights = []\n",
    "for l in [conv1, conv2, conv3, conv4, conv5, conv6, fc1, fc2]:\n",
    "    ws = model.get_weights(l.W)\n",
    "    bs = model.get_weights(l.b)\n",
    "    all_weights.append([ws, bs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Keras model, and transfer the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = Sequential([\n",
    "    Conv2D(32, 3, input_shape=(32, 32, 3), activation='relu', padding=\"same\"),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Conv2D(64, 3, activation='relu', padding=\"same\"),\n",
    "\n",
    "    # Step 4: Convolution yet again\n",
    "    Conv2D(64, 3, activation='relu', padding=\"same\"),\n",
    "    Conv2D(64, 3, activation='relu', padding=\"same\"),\n",
    "    Conv2D(64, 3, activation='relu', padding=\"same\"),\n",
    "    Conv2D(64, 3, activation='relu', padding=\"same\"),\n",
    "\n",
    "    # Step 5: Max pooling again\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    \n",
    "    # Step 6: Fully-connected 512 node neural network\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.25),  # Different from COAP, as the tf states how much to keep, but keras is the dropout rate\n",
    "    Dense(len(class_labels), activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the model, we needed to have values parameters set for loss, so just setting these with the same as the TFL model, although we won't train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.compile(keras.optimizers.Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the weights, we see that they are a len()=2 list with the weights, then bias. Both of those are then np.arrays.\n",
    "ws = keras_model.layers[0].get_weights()\n",
    "print(type(ws), type(ws[0]))\n",
    "print(len(ws))\n",
    "\n",
    "# For nontrainable layers they are empty lists:\n",
    "ws = keras_model.layers[1].get_weights()\n",
    "print(type(ws))\n",
    "print(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually setting weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [conv1, conv2, conv3, conv4, conv5, conv6, fc1, fc2]\n",
    "keras_model.layers[0].set_weights(all_weights[0])\n",
    "keras_model.layers[2].set_weights(all_weights[1])\n",
    "keras_model.layers[3].set_weights(all_weights[2])\n",
    "keras_model.layers[4].set_weights(all_weights[3])\n",
    "keras_model.layers[5].set_weights(all_weights[4])\n",
    "keras_model.layers[6].set_weights(all_weights[5])\n",
    "keras_model.layers[9].set_weights(all_weights[6])\n",
    "keras_model.layers[11].set_weights(all_weights[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.save('keras_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the 'transferred' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras_model = keras.models.load_model('keras_model.h5')\n",
    "loaded_model = keras.models.load_model('keras_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in class_labels:\n",
    "    acc = keras_correct_positives(c, loaded_model)\n",
    "    print(c, acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "oil 99.40387481371089\n",
    "other 98.08389435525635\n",
    "bubble 99.96206373292868\n",
    "faecal_pellets 96.69260700389106\n",
    "copepod 99.39117199391173\n",
    "diatom_chain 98.3529411764706\n",
    "oily_gas 97.7035490605428 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some other checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "print(keras_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A some what surpurflous check that the weights are the same:\n",
    "# [conv1, conv2, conv3, conv4, conv5, conv6, fc1, fc2]\n",
    "\n",
    "assert (model.get_weights(conv1.W) - keras_model.layers[0].get_weights()[0]).sum() == 0, \"Oh no, you did an error\"\n",
    "assert (model.get_weights(conv1.b) - keras_model.layers[0].get_weights()[1]).sum() == 0, \"Oh no, you did an error\"\n",
    "\n",
    "assert (model.get_weights(conv3.W) - keras_model.layers[3].get_weights()[0]).sum() == 0, \"Oh no, you did an error\"\n",
    "assert (model.get_weights(conv3.b) - keras_model.layers[3].get_weights()[1]).sum() == 0, \"Oh no, you did an error\"\n",
    "\n",
    "assert (model.get_weights(conv6.W) - keras_model.layers[6].get_weights()[0]).sum() == 0, \"Oh no, you did an error\"\n",
    "assert (model.get_weights(conv6.b) - keras_model.layers[6].get_weights()[1]).sum() == 0, \"Oh no, you did an error\"\n",
    "\n",
    "assert (model.get_weights(fc1.W) - keras_model.layers[9].get_weights()[0]).sum() == 0, \"Oh no, you did an error\"\n",
    "assert (model.get_weights(fc1.b) - keras_model.layers[9].get_weights()[1]).sum() == 0, \"Oh no, you did an error\"\n",
    "\n",
    "assert (model.get_weights(fc2.W) - keras_model.layers[11].get_weights()[0]).sum() == 0, \"Oh no, you did an error\"\n",
    "assert (model.get_weights(fc2.b) - keras_model.layers[11].get_weights()[1]).sum() == 0, \"Oh no, you did an error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some checked I needed when debugging:\n",
    "# files = glob.glob(os.path.join(DATABASE_PATH, 'oil', '*.tiff'))\n",
    "# img = imread(files[4])\n",
    "# img = scipy.misc.imresize(img, (32, 32), interp=\"bicubic\").astype(np.float32, casting='unsafe')\n",
    "# img = np.expand_dims(img, 0)\n",
    "\n",
    "# pp_img = img.copy()\n",
    "# pp_img = pp_img / (pp_img.std() / 1.186766)\n",
    "# pp_img = pp_img - (pp_img.mean() + 0.50608945 )\n",
    "# print(pp_img.mean(), pp_img.std())\n",
    "# print('Keras prediction:   ', keras_model.predict(pp_img))\n",
    "# print('TFLearn prediction: ', model.predict(img))\n",
    "# print('Keras prediction:   ', keras_model.predict(img))\n",
    "# print(img.mean(), img.std())\n",
    "\n",
    "# print(keras_model)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (silcam-dev)",
   "language": "python",
   "name": "silcam-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
